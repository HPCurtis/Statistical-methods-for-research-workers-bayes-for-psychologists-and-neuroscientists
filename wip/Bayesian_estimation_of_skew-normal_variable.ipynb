{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90567ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the neccesary python libraries for Bayesian analysis\n",
    "# and data visualisation.\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import stan as ps\n",
    "import arviz as az\n",
    "import statistics as Stats\n",
    "from patsy import dmatrix\n",
    "import os\n",
    "\n",
    "# Importing nest_asyncio is only necessary to run pystan in Jupyter Notebooks.\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply();\n",
    "\n",
    "# Universal variables\n",
    "pp_samples = 100\n",
    "# Specify the number of chains to the number of availible cpu's.\n",
    "n_chains = os.cpu_count()\n",
    "n_samples = 2000\n",
    "#Convert to int so Stan will not crash below.\n",
    "n_warmup = int(n_samples/2)\n",
    "#Specify step size.\n",
    "stepS = .8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99869348",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML as Center\n",
    "\n",
    "Center(\"\"\" <style>\n",
    ".output_png {\n",
    "    display: table-cell;\n",
    "    text-align: center;\n",
    "    vertical-align: middle;\n",
    "}\n",
    "</style> \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e131a088",
   "metadata": {},
   "source": [
    "# Bayesian estimation of skew normal variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c81b90e",
   "metadata": {},
   "source": [
    "The following notebook demonstrates a very useful but simple example of an application of Bayesian data analysis. That application being that if their is a parameterised probability distribution and you have enough data fitting a statistical model to that data based on that probability distribution in the form of a likelihood is feasible and therefore, so is statistical inference. Remember Bayesian data analysis is simply the application of Bayes rule, $P(\\theta \\mid y) = \\large \\frac{P(y \\mid \\theta) \\, P(\\theta)}{P(y)}$ to statistical models Gelman et al. (2013).\n",
    "\n",
    "This may not be a surprise to any researchers who understand and apply maximum likelihood methods. However, that is a skillset not possessed and arguably not even within the purview of many psychological researchers who are taught statistical tests based or gaussian assumptions (Delacre, Lakens, & Leys, 2017, McElreath, 2020) and thus dominates their statistical practices. A primary advantage for researchers which this notebook hopes to show in a simple form is the use Bayesian data analysis to build models that describe the data generating process for the phenomenon under study (Kruschke, 2015; McElreath, 2020).\n",
    "\n",
    "Therefore, the example below demonstrates the fitting of a simple skew-normal model to some multiple observations of a single subjects reaction time data to a stroop task. In this example many researcher would apply a Null hypothesis significance test (NHST) likely in the form of one sample t-test or potentially even non-parametric test when the data is skewed like the data here (Lachaud & Renaud, 2011) with various justifications. The exact issues with this are diverse and therefore not explained in any further detail here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ae8779",
   "metadata": {},
   "source": [
    "# Steps of Bayesian data analysis\n",
    "\n",
    "<font size = \"3\"> Kruschke (2015) offers a step by step formulation for how to conduct a Bayesian analysis:\n",
    "\n",
    "1. Identify the relevant data for the question under investigation.\n",
    "\n",
    "2. Define the descriptive (mathematical) model for the data.\n",
    "\n",
    "3. Specify the Priors for the model. If scientific research publication is the goal the priors must be accepted by a skeptical audience. Much of this can be achieved using prior predictive checks to ascertain if the priors are reasonable.\n",
    "\n",
    "4. Using Bayes' rule estimate the posterior for the parameters of the model using the likelihood and priors. Then interpret the posterior.\n",
    "\n",
    "5. Conduct model checks. i.e. Posterior predictive checks.</font> \n",
    "\n",
    "<font size = \"1\">This notebook will follow this approach generally.</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864847ca",
   "metadata": {},
   "source": [
    "#  Step 1 - Identify the relevant data for  the question under investigation\n",
    "\n",
    "The data that is analysed below actually comes from another Bayesian teaching resource https://vasishth.github.io/bayescogsci/book/ex-compbda.html. It is actually suggested as an end of chapter exercise. The author of this notebook openly admits they were struggling to find a nice example for a skew normal model to show a not so common application of Bayesian inference (with out simple simulatinf) and thanks the authors for the inspiration. The authors of that textbook as far as the author of this notebook is aware advocate for open science/education and will not have an issues with the use of their data. Of course, if their are any issues please make the author of this notebook aware. Anyhow, the data is imported from the associated github page for online textbook linked above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cb64c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Url variable for the raw data for the reaction times of the single participant.\n",
    "url = 'https://raw.githubusercontent.com/bnicenboim/bcogsci/master/data-raw/datasets/button_press.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad69b4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate pandas dataframe for the reaction time dataset\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b846ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the headers of the dataset.\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e04e3bb",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec807c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the reaction time data for the single participants repeated trial observations.\n",
    "sns.displot(df['rt']).set(title = 'Single particpants Stroop task trial reaction times scores', \n",
    "                          xlabel = 'Reaction time');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0382d68d",
   "metadata": {},
   "source": [
    "# Step 2 - Define the descriptive statistical model \\begin{align*}\n",
    "y_{i} &\\sim SkewNormal(\\xi, \\omega, \\alpha)\n",
    "\\\\ \\xi &\\sim Normal(\\bar{y}, \\sigma_y * 5)\n",
    "\\\\ \\omega &\\sim Normal(0, \\sigma_y * 10)\n",
    "\\\\ \\alpha &\\sim Normal(0, 10)\n",
    "\\end{align*} \n",
    "\n",
    "\n",
    "### Step 3 - Specifying priors\n",
    "The prior values here were selected to be broad and data informed in line with the suggestions of prior setting in Kruschke (2015). This is not such a problem here as there is large number of data points and the models are simple (low-dimensional)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932f2120",
   "metadata": {},
   "source": [
    "## Stan model for skew-normal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9124658f",
   "metadata": {},
   "outputs": [],
   "source": [
    "skew_normal = '''\n",
    "\n",
    "// Functions defined here are simple but help demonstrate \n",
    "// and clean up the generated quantities block\n",
    "// which is generaly appropriate with writing\n",
    "// Clearer Stan code. \n",
    "\n",
    "functions{\n",
    "// Function to calculate the expected value of skew normal random variable. \n",
    "real skew_normal_mean(real xi, real omega, real alpha){\n",
    "    real delta = alpha / sqrt(1 + pow(alpha, 2));\n",
    "    real mean_sn = xi + omega * delta * sqrt(2/pi());\n",
    "    return mean_sn;\n",
    "    }\n",
    "}\n",
    "\n",
    "data{\n",
    "\n",
    "int<lower = 1> N; // Set total observations\n",
    "vector[N] y; // vector of dependent variable values\n",
    "\n",
    "// Variable to specify if prior or posterior predictive checks are run.\n",
    "int<lower = 0, upper = 1> onlyprior;\n",
    "\n",
    "// Prior parameter values\n",
    "real xi_mu;\n",
    "real xi_sigma;\n",
    "real omega_sigma;\n",
    "real alpha_sigma;\n",
    "}\n",
    "parameters{\n",
    "\n",
    "// Parameters for Stan parameterisation of skew normal distribution\n",
    "real xi;\n",
    "real<lower = 0> omega;\n",
    "real alpha;\n",
    "\n",
    "}\n",
    "\n",
    "model{\n",
    "// priors\n",
    "xi ~ normal(xi_mu,xi_sigma);\n",
    "omega ~ normal(0, omega_sigma);\n",
    "// This prior represents that the skew could be in either left/right.\n",
    "alpha ~ normal(0, alpha_sigma);\n",
    "\n",
    "// Run likelihoood if onlyprior is set to 0\n",
    "if(!onlyprior){\n",
    "    y ~ skew_normal(xi, omega, alpha);\n",
    "    }\n",
    "}\n",
    "\n",
    "generated quantities{\n",
    "vector[N] yrep;\n",
    "vector[N] log_lik;\n",
    "real sn_mean;\n",
    "\n",
    "sn_mean = skew_normal_mean(xi, omega, alpha);\n",
    "\n",
    "// Need for loop as skew_normal_rng is not vectorisable. See Stan manual for updates\n",
    "for(i in 1:N){\n",
    "    yrep[i] = skew_normal_rng(xi, omega, alpha);\n",
    "    }\n",
    "\n",
    "// Only run log likelihood when posterior is estimated    \n",
    "if(!onlyprior){\n",
    "for (i in 1:N) {\n",
    "    log_lik[i] = skew_normal_lpdf(y[i] | xi, omega, alpha);\n",
    "      }\n",
    "  }  \n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddf5a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate python dictionary to be passed to stan during build of the model for later sampling.\n",
    "data_sn = {'N': len(df),\n",
    "        'y': df['rt'].values,\n",
    "        #Set as 1 fro prior predictive checks\n",
    "        'onlyprior': 0,\n",
    "        'xi_mu': np.mean(df['rt']),\n",
    "        'xi_sigma': np.std(df['rt']) * 5,\n",
    "        'omega_sigma': np.std(df['rt']) * 10,\n",
    "        'alpha_sigma': 10.,\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0081e10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile stan model tp C++ code\n",
    "sm = ps.build(skew_normal, data = data_sn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a7c1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from the Stan model.\n",
    "fit = sm.sample(num_chains = n_chains , num_samples = n_samples, num_warmup = n_warmup, stepsize = stepS);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681d944a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geneate panda data frame of the posterior samples with metrics calculated in addition.\n",
    "fit_dfsn = fit.to_frame()\n",
    "\n",
    "# Calculate the number of divergences\n",
    "np.sum(fit_dfsn['divergent__'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91db587c",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(fit, var_names = ['xi', 'omega', 'alpha', 'sn_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1f8f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_posterior(fit, var_names = ['xi', 'omega', 'alpha', 'sn_mean']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61391d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the arviz package the traceplots of the 4 MCMC chains can be plotted.\n",
    "az.plot_trace(fit, var_names = (\"xi\", \"omega\", \"alpha\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637e5067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the arviz package generate rank plot for the MCMC chains to assess mixing.\n",
    "az.plot_rank(fit, var_names = (\"xi\", \"omega\", \"alpha\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c3686a",
   "metadata": {},
   "source": [
    "The rank plots suggest little deviance from uniforminity of the chains. Suggesting that they have mixed well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91255db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pystan fit object to IO for Arviz functions.\n",
    "idata = az.from_pystan(posterior = fit, posterior_model = sm, log_likelihood = ['log_lik'], posterior_predictive = ['yrep'], observed_data= 'y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b501f494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot posterior simulated data sets for posterior predictive check\n",
    "if data_sn['onlyprior'] == 0:\n",
    "    az.plot_ppc( idata, data_pairs= {\"y\" : \"yrep\"}, num_pp_samples = pp_samples, observed = True );\n",
    "else:\n",
    "    az.plot_ppc( idata, data_pairs= {\"y\" : \"yrep\"}, num_pp_samples= pp_samples, observed = False );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141ebcdd",
   "metadata": {},
   "source": [
    "Posterior predcitive plots show that the skew normal model capture the participants reaction times fairly well. However, it fails to capture the outlier reaction time outlier score. This is a result of the skew-normal unsurprisingly still have the skinny tail of the regular normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee554082",
   "metadata": {},
   "source": [
    "# Can we do better?\n",
    "\n",
    "## Lognormal model for reaction times\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e79ecab",
   "metadata": {},
   "source": [
    "# Define the descriptive statistical model \\begin{align*}\n",
    "y_{i} &\\sim LogNormal(\\mu, \\sigma)\n",
    "\\\\ \\mu &\\sim Normal(6, 1.5)\n",
    "\\\\ \\sigma &\\sim Normal(0, 1)\n",
    "\\end{align*} \n",
    "\n",
    "The prior values here were selected to be consistent with the ones used in https://vasishth.github.io/bayescogsci/book/ex-compbda.html. The amount of data here again makes the priors less of a concern in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f79180",
   "metadata": {},
   "source": [
    "### Stan model for lognormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9341898",
   "metadata": {},
   "outputs": [],
   "source": [
    "ln_model = '''\n",
    "// Functions defined here are simple but help demonstrate \n",
    "// and clean up the generated quantities block\n",
    "// which is generaly appropriate with stan code. \n",
    "\n",
    "functions{\n",
    "real logn_median(real mu){\n",
    "    return exp(mu);\n",
    "}\n",
    "real logn_mean(real mu , real sigma){\n",
    "    return exp(mu + pow(sigma, 2) / 2);\n",
    "    }\n",
    "}\n",
    "\n",
    "data{\n",
    "\n",
    "int<lower = 1> N; // Set total observations\n",
    "vector[N] y; // vector of dependent variable values\n",
    "\n",
    "// Variable to specify if prior or posterior predictive checks are run.\n",
    "int<lower = 0, upper = 1> onlyprior;\n",
    "\n",
    "// Prior parameter values\n",
    "real mu_mu;\n",
    "real mu_sigma;\n",
    "real sigma_sigma;\n",
    "}\n",
    "parameters{\n",
    "\n",
    "// Parameters for Stan parameterisation of lognormal distribution\n",
    "real mu;\n",
    "real<lower = 0> sigma;\n",
    "\n",
    "}\n",
    "\n",
    "model{\n",
    "\n",
    "// Priors\n",
    "mu ~ normal(mu_mu, mu_sigma);\n",
    "sigma ~ normal(0, sigma_sigma);\n",
    "\n",
    "// Run likelihoood if onlyprior is set to 0\n",
    "if(!onlyprior){\n",
    "    y ~ lognormal(mu, sigma);\n",
    "    }\n",
    "}\n",
    "\n",
    "generated quantities{\n",
    "vector[N] yrep;\n",
    "vector[N] log_lik;\n",
    "\n",
    "// Calculate the posterior for the median value of a lognormal random variable.\n",
    "real median_ln = logn_median(mu);\n",
    "\n",
    "// Calculate the posterior for the mean value of a lognormal random variable.\n",
    "real mean_ln = logn_mean(mu , sigma);\n",
    "\n",
    "// Need for loop as skew_normal_rng is not vectorisable. see Stan manuals for updates\n",
    "for(i in 1:N){\n",
    "    yrep[i] = lognormal_rng(mu, sigma);\n",
    "    }\n",
    "    \n",
    "// Only run log likelihood when posterior is estimated    \n",
    "if(!onlyprior){\n",
    "for (i in 1:N) {\n",
    "    log_lik[i] = lognormal_lpdf(y[i] | mu, sigma);\n",
    "      }\n",
    "  }\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd476dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ln = {'N': len(df),\n",
    "           'y': df['rt'].values,\n",
    "           # Set to 0 to run likelihood and fit the model \n",
    "           'onlyprior': 0,\n",
    "           'mu_mu': 6.,\n",
    "           'mu_sigma': 1.5,\n",
    "           'sigma_sigma': 1\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50d3e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the lognormal Stan model to C++.\n",
    "sm_ln = ps.build(ln_model, data = data_ln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d3a955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smaple the psoterios for the lognormal model.\n",
    "fit_ln = sm_ln.sample(num_chains = n_chains , num_samples = n_samples, num_warmup = n_warmup, stepsize = stepS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7640533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output summary of model parameters\n",
    "az.summary(fit_ln, var_names = ['mu', 'sigma', 'median_ln', 'mean_ln'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc459659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate pandas dataframe with posterior samples and key metrics calculated.\n",
    "fit_lndf = fit_ln.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31ab29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of divergences\n",
    "np.sum(fit_lndf['divergent__'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ab37bf",
   "metadata": {},
   "source": [
    "## Expected value of lognormal variable posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb83c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_posterior(fit_ln, var_names = ['median_ln', 'mean_ln']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0009ab2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(fit_ln, var_names = ['mu', 'sigma']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1efe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_rank(fit_ln, var_names = (\"mu\", \"sigma\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45abfe44",
   "metadata": {},
   "outputs": [],
   "source": [
    "idata_ln = az.from_pystan(posterior = fit_ln, posterior_model = sm_ln, log_likelihood = ['log_lik'],\n",
    "                          posterior_predictive=['yrep'],observed_data= 'y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e371853a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_ln['onlyprior'] == 0:\n",
    "    az.plot_ppc( idata_ln, data_pairs= {\"y\" : \"yrep\"}, num_pp_samples= pp_samples, observed = True );\n",
    "else:\n",
    "    az.plot_ppc( idata_ln, data_pairs= {\"y\" : \"yrep\"}, num_pp_samples= pp_samples, observed = False );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e01c62",
   "metadata": {},
   "source": [
    "### How does a normal distribution model fair?\n",
    "\n",
    "To coincide with what most psychological researchers are used to applying it is useful to see how a normal distributional model fairs when modelling this data set of reaction times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93cf6c2",
   "metadata": {},
   "source": [
    "## Define the descriptive statistical model \\begin{align*}\n",
    "y_{i} &\\sim Normal(\\mu, \\sigma)\n",
    "\\\\ \\mu &\\sim Normal(\\bar{y}, \\sigma_y * 5)\n",
    "\\\\ \\sigma &\\sim Normal(0, \\sigma_y * 10)\n",
    "\\end{align*} \n",
    "\n",
    "The prior values here were selected to coincide with the skew normal model as close as possible. Of course, the skewness parameter ($\\alpha$) makes a large difference to the model skew-normal model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b230c58",
   "metadata": {},
   "source": [
    "## Stan model for Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b38bfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_model = '''\n",
    "data{\n",
    "\n",
    "int<lower = 1> N; // Set total observations\n",
    "vector[N] y; // vector of dependent variable values\n",
    "\n",
    "// Variable to specify if prior or posterior predictive checks are run.\n",
    "int<lower = 0, upper = 1> onlyprior;\n",
    "\n",
    "// Prior parameter values\n",
    "real mu_mu;\n",
    "real mu_sigma;\n",
    "real sigma_sigma;\n",
    "}\n",
    "parameters{\n",
    "\n",
    "// Parameters for Stan parameterisation of lognormal distribution\n",
    "real mu;\n",
    "real<lower = 0> sigma;\n",
    "\n",
    "}\n",
    "\n",
    "model{\n",
    "// Priors\n",
    "mu ~ normal(mu_mu, mu_sigma);\n",
    "sigma ~ normal(0, sigma_sigma);\n",
    "\n",
    "// Run likelihoood if onlyprior is set to 0\n",
    "if(!onlyprior){\n",
    "    y ~ normal(mu, sigma);\n",
    "    }\n",
    "}\n",
    "\n",
    "generated quantities{\n",
    "vector[N] yrep;\n",
    "\n",
    "vector[N] log_lik;\n",
    "for (i in 1:N) {\n",
    "    log_lik[i] = normal_lpdf(y[i] | mu, sigma);\n",
    "      }\n",
    "\n",
    "for(i in 1:N){\n",
    "    yrep[i] = normal_rng(mu, sigma);\n",
    "    }\n",
    "\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca4c280",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_n = {'N': len(df),\n",
    "           'y': df['rt'].values,\n",
    "           'onlyprior': 0,\n",
    "           'mu_mu': np.mean(df['rt']),\n",
    "           'mu_sigma': np.std(df['rt']) * 5,\n",
    "           'sigma_sigma': np.std(df['rt']) * 10,\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e860318d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_n = ps.build(n_model, data = data_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acb0234",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_n = sm_n.sample(num_chains = n_chains , num_samples = n_samples, num_warmup = n_warmup, stepsize = stepS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13a139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(fit_n, var_names = ['mu', 'sigma'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfc291e",
   "metadata": {},
   "source": [
    "# Traceplots normal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b04c108",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_posterior(fit_n, var_names = ['mu', 'sigma']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdca6ded",
   "metadata": {},
   "source": [
    "# Traceplots normal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ca4fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(fit_n, var_names = ['mu', 'sigma']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fff51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_rank(fit_n, var_names = (\"mu\", \"sigma\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f0a88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pystan fit object to IO for Arviz functions.\n",
    "# Convert pystan fit object to IO for Arviz functions.\n",
    "idata_n = az.from_pystan(posterior=fit_n, posterior_model = sm_n, log_likelihood = ['log_lik'], \n",
    "                       posterior_predictive=['yrep'],observed_data= 'y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820fd14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_n['onlyprior'] == 0:\n",
    "    az.plot_ppc( idata_n, data_pairs= {\"y\" : \"yrep\"}, num_pp_samples= pp_samples, observed = True );\n",
    "else:\n",
    "    az.plot_ppc( idata_n, data_pairs= {\"y\" : \"yrep\"}, num_pp_samples= pp_samples, observed = False );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4af3a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate python dictionary to label model with the correct data.\n",
    "compare_dict = {\"Model_SN\": idata, \"Model_LN\": idata_ln, \"Model_N\": idata_n}\n",
    "\n",
    "# Compare models using psis leave one out cross validated.\n",
    "# Output model comparison results.\n",
    "model_compare = az.compare(compare_dict, scale = 'deviance');\n",
    "print(model_compare)\n",
    "az.plot_compare(model_compare);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0587eee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "expectations = [fit[\"sn_mean\"], fit_ln[\"mean_ln\"], fit_n[\"mu\"] ]\n",
    "expectation_labs = [\"Model_SN\", \"Model_LN\" , \"Model_N\"]\n",
    "for i in range(0,len(expectations)):\n",
    "    az.plot_posterior(expectations[i])\n",
    "    plt.title(expectation_labs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250515c8",
   "metadata": {},
   "source": [
    "The model comparison results above show some interesting things. Firstly, the performance of the models in terms of out of sample prediction are not surprsing seeing as they are attempting to model reaction time data with the lognormal model performing the best. However, as the model comparison plot shows that this is only in terms of point estimation as there is a lot of overlap in terms of the estimated error and that uncertainty increases as you go from LN-SN-N model. Another interesting outcome to look at with the model comparison results is that SN and N model both resulted in warnings for highly leveraging points where as LN did not. Which suggests that LN model was more robust to the outlier value. What is also of interest in that the estiamtes of each model in terms of expectations were very similar\n",
    "\n",
    "This notebook has hopefully shown that Bayesian data analysis offers robust and flexible modelling options. In addition, it should hopefully show that even in this simple situation more appropriate modelling options exist and that there are alternatives to the general dominance of gaussian assumptions within standard statistical tests. It has to be said though that application of these alternatives is often not as simple as gaussian models. In that the gaussian distribution is nice, in terms that it is easier to fit to data (computationally) and requires less post work to understand. However, we can likely do better in situations that call for these alternatives. Additionally, we do not have to commit to a single model and the studying multiple models is likely to be fruitful in understanding psychological phenomena (Smaldino, 2017)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0826942",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Delacre, M., Lakens, D., & Leys, C. (2017). Why psychologists should by default use Welch’s t-test instead of Student’s t-test. International Review of Social Psychology, 30(1).\n",
    "\n",
    "Kruschke, J. K., (2015). Doing bayesian data analysis: A tutorial with R, JAGS, and Stan. New york, NY: Academic Press.\n",
    "\n",
    "Lachaud, C. M., & Renaud, O. (2011). A tutorial for analyzing human reaction times: How to filter data, manage missing values, and choose a statistical model. Applied Psycholinguistics, 32(2), 389-416.\n",
    "\n",
    "McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan. Boca Raton: CRC Press.\n",
    "\n",
    "Smaldino, P. E. (2017). Models are stupid, and we need more of them. Computational social psychology, 311-331."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4424353",
   "metadata": {},
   "source": [
    "## Exercises for the motivated readers\n",
    "\n",
    "1. Change the prior parameter values and onlyprior variable (set to 1) in the python dictionaries for the models and see the effects on prior predictive distributions.\n",
    "\n",
    "2. Fit a student-t distribution model and do model comparison to see the outcomes.\n",
    "\n",
    "3. Use tighter priors on the models and observe the outcomes for posterior predictive checks and LOO estimates."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
